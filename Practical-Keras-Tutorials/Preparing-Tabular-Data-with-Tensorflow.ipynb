{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fa9793f",
   "metadata": {
    "papermill": {
     "duration": 0.036532,
     "end_time": "2021-08-19T11:59:08.809825",
     "exception": false,
     "start_time": "2021-08-19T11:59:08.773293",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Preparing Tabular Data with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946f411d",
   "metadata": {
    "papermill": {
     "duration": 0.0349,
     "end_time": "2021-08-19T11:59:08.880081",
     "exception": false,
     "start_time": "2021-08-19T11:59:08.845181",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Tabular data consist of rows and columns. The values of the categorical columns have to encode as one-hot encoding. In this tutorail, I am going to cover how to preparing tabular data. To show this, I'll use Titanic dataset. First of all, let's import libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9ad97fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-19T11:59:08.954980Z",
     "iopub.status.busy": "2021-08-19T11:59:08.953859Z",
     "iopub.status.idle": "2021-08-19T11:59:16.123428Z",
     "shell.execute_reply": "2021-08-19T11:59:16.124007Z",
     "shell.execute_reply.started": "2021-08-18T18:20:59.755356Z"
    },
    "papermill": {
     "duration": 7.209421,
     "end_time": "2021-08-19T11:59:16.124305",
     "exception": false,
     "start_time": "2021-08-19T11:59:08.914884",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Importing libraries.\n",
    "import functools\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e176a6db",
   "metadata": {
    "papermill": {
     "duration": 0.034682,
     "end_time": "2021-08-19T11:59:16.194036",
     "exception": false,
     "start_time": "2021-08-19T11:59:16.159354",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Loading the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fd0664",
   "metadata": {
    "papermill": {
     "duration": 0.034005,
     "end_time": "2021-08-19T11:59:16.262459",
     "exception": false,
     "start_time": "2021-08-19T11:59:16.228454",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The Titanic dataset is open source and tabular dataset. This dataset consist of columns as such age, gender, cabin grade, and whether or not they survived. Google provide this dataset. Let me create variables that contain URLs of train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "334b66d7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-19T11:59:16.334835Z",
     "iopub.status.busy": "2021-08-19T11:59:16.334162Z",
     "iopub.status.idle": "2021-08-19T11:59:16.338270Z",
     "shell.execute_reply": "2021-08-19T11:59:16.338810Z",
     "shell.execute_reply.started": "2021-08-18T18:21:07.771165Z"
    },
    "papermill": {
     "duration": 0.041956,
     "end_time": "2021-08-19T11:59:16.338983",
     "exception": false,
     "start_time": "2021-08-19T11:59:16.297027",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Creating variables for urls of datasets.\n",
    "TRAIN_DATA_URL = \"https://storage.googleapis.com/tf-datasets/titanic/train.csv\"\n",
    "TEST_DATA_URL = \"https://storage.googleapis.com/tf-datasets/titanic/eval.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0861ac",
   "metadata": {
    "papermill": {
     "duration": 0.033889,
     "end_time": "2021-08-19T11:59:16.407133",
     "exception": false,
     "start_time": "2021-08-19T11:59:16.373244",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "I am going to use get_files() method which downloads a file from a URL if it not already in the cache."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "762410d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-19T11:59:16.478883Z",
     "iopub.status.busy": "2021-08-19T11:59:16.478218Z",
     "iopub.status.idle": "2021-08-19T11:59:17.461589Z",
     "shell.execute_reply": "2021-08-19T11:59:17.462257Z",
     "shell.execute_reply.started": "2021-08-18T18:21:07.777102Z"
    },
    "papermill": {
     "duration": 1.021258,
     "end_time": "2021-08-19T11:59:17.462514",
     "exception": false,
     "start_time": "2021-08-19T11:59:16.441256",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tf-datasets/titanic/train.csv\n",
      "32768/30874 [===============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tf-datasets/titanic/eval.csv\n",
      "16384/13049 [=====================================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# Creating variables for paths of datasets.\n",
    "train_file_path = tf.keras.utils.get_file(\"train.csv\", TRAIN_DATA_URL)\n",
    "test_file_path = tf.keras.utils.get_file(\"eval.csv\",  TEST_DATA_URL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332cecfa",
   "metadata": {
    "papermill": {
     "duration": 0.036171,
     "end_time": "2021-08-19T11:59:17.541346",
     "exception": false,
     "start_time": "2021-08-19T11:59:17.505175",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Pandas is the most popular library of Python. You can manipulate dataset with Pandas. To read these datasets, you can use read_csv () method in Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9765b9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-19T11:59:17.621244Z",
     "iopub.status.busy": "2021-08-19T11:59:17.620529Z",
     "iopub.status.idle": "2021-08-19T11:59:17.636285Z",
     "shell.execute_reply": "2021-08-19T11:59:17.635672Z",
     "shell.execute_reply.started": "2021-08-18T18:21:08.086992Z"
    },
    "papermill": {
     "duration": 0.058119,
     "end_time": "2021-08-19T11:59:17.636469",
     "exception": false,
     "start_time": "2021-08-19T11:59:17.578350",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Converting train_file_path into pandas dataframe.\n",
    "train_df = pd.read_csv(train_file_path, header='infer')\n",
    "test_df = pd.read_csv(test_file_path, header='infer')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee09322f",
   "metadata": {
    "papermill": {
     "duration": 0.036868,
     "end_time": "2021-08-19T11:59:17.710739",
     "exception": false,
     "start_time": "2021-08-19T11:59:17.673871",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let me take a look the first five rows of train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28c0cd4b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-19T11:59:17.788123Z",
     "iopub.status.busy": "2021-08-19T11:59:17.787464Z",
     "iopub.status.idle": "2021-08-19T11:59:17.823173Z",
     "shell.execute_reply": "2021-08-19T11:59:17.822614Z",
     "shell.execute_reply.started": "2021-08-18T18:21:08.111411Z"
    },
    "papermill": {
     "duration": 0.075256,
     "end_time": "2021-08-19T11:59:17.823330",
     "exception": false,
     "start_time": "2021-08-19T11:59:17.748074",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>n_siblings_spouses</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>class</th>\n",
       "      <th>deck</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>Third</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>First</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>Third</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>First</td>\n",
       "      <td>C</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>male</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>Third</td>\n",
       "      <td>unknown</td>\n",
       "      <td>Queenstown</td>\n",
       "      <td>y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived     sex   age  n_siblings_spouses  parch     fare  class     deck  \\\n",
       "0         0    male  22.0                   1      0   7.2500  Third  unknown   \n",
       "1         1  female  38.0                   1      0  71.2833  First        C   \n",
       "2         1  female  26.0                   0      0   7.9250  Third  unknown   \n",
       "3         1  female  35.0                   1      0  53.1000  First        C   \n",
       "4         0    male  28.0                   0      0   8.4583  Third  unknown   \n",
       "\n",
       "   embark_town alone  \n",
       "0  Southampton     n  \n",
       "1    Cherbourg     n  \n",
       "2  Southampton     y  \n",
       "3  Southampton     n  \n",
       "4   Queenstown     y  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Take a look titanic dataset.\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b45bc0",
   "metadata": {
    "papermill": {
     "duration": 0.039058,
     "end_time": "2021-08-19T11:59:17.899675",
     "exception": false,
     "start_time": "2021-08-19T11:59:17.860617",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Preprocessing the Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4006020e",
   "metadata": {
    "papermill": {
     "duration": 0.038216,
     "end_time": "2021-08-19T11:59:17.976878",
     "exception": false,
     "start_time": "2021-08-19T11:59:17.938662",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "As you can see above dataseti dataset consist of numeric and categorical columns. You will need to mark \"survived\" columns as the target and mark the rest of the columns as features. To do this I am going to use tf.data.experimental.make_csv_dataset() method. This method reads CSV files into a dataset, where each element of the dataset is a (features, labels) tuple that corresponds to a batch of CSV rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00d98840",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-19T11:59:18.066350Z",
     "iopub.status.busy": "2021-08-19T11:59:18.057321Z",
     "iopub.status.idle": "2021-08-19T11:59:18.249373Z",
     "shell.execute_reply": "2021-08-19T11:59:18.249887Z",
     "shell.execute_reply.started": "2021-08-18T18:21:08.154983Z"
    },
    "papermill": {
     "duration": 0.235847,
     "end_time": "2021-08-19T11:59:18.250095",
     "exception": false,
     "start_time": "2021-08-19T11:59:18.014248",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Creating the target and featrues variables.\n",
    "LABEL_COLUMN = 'survived'\n",
    "LABELS = [0, 1]\n",
    "# Let's specify file path, batch size, label name, missing value parameters in make_csv_dataset method.\n",
    "train_ds = tf.data.experimental.make_csv_dataset(\n",
    "        train_file_path,\n",
    "        batch_size = 3,\n",
    "        label_name=LABEL_COLUMN,\n",
    "        na_value=\"?\",\n",
    "        num_epochs= 1,\n",
    "        ignore_errors=True)\n",
    "# Let's create test dataset as above.\n",
    "test_ds = tf.data.experimental.make_csv_dataset(\n",
    "        test_file_path,\n",
    "        batch_size=3,\n",
    "        label_name=LABEL_COLUMN,\n",
    "        na_value=\"?\",\n",
    "        num_epochs=1,\n",
    "        ignore_errors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e1808f",
   "metadata": {
    "papermill": {
     "duration": 0.036761,
     "end_time": "2021-08-19T11:59:18.324022",
     "exception": false,
     "start_time": "2021-08-19T11:59:18.287261",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's take a look columns of train dataset in the first batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f430d87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-19T11:59:18.401448Z",
     "iopub.status.busy": "2021-08-19T11:59:18.400752Z",
     "iopub.status.idle": "2021-08-19T11:59:18.530902Z",
     "shell.execute_reply": "2021-08-19T11:59:18.530336Z",
     "shell.execute_reply.started": "2021-08-18T18:21:08.369986Z"
    },
    "papermill": {
     "duration": 0.169805,
     "end_time": "2021-08-19T11:59:18.531068",
     "exception": false,
     "start_time": "2021-08-19T11:59:18.361263",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([1 0 0], shape=(3,), dtype=int32)\n",
      "sex: [b'female' b'male' b'male']\n",
      "age: [31. 28. 64.]\n",
      "n_siblings_spouses: [1 0 1]\n",
      "parch: [1 0 4]\n",
      "fare: [ 20.525  15.05  263.   ]\n",
      "class: [b'Third' b'Second' b'First']\n",
      "deck: [b'unknown' b'unknown' b'C']\n",
      "embark_town: [b'Southampton' b'Cherbourg' b'Southampton']\n",
      "alone: [b'n' b'y' b'n']\n"
     ]
    }
   ],
   "source": [
    "for batch, label in train_ds.take(1):\n",
    "    print(label)\n",
    "    for key, value in batch.items():\n",
    "        print(f\"{key}: {value.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86c3a25",
   "metadata": {
    "papermill": {
     "duration": 0.037709,
     "end_time": "2021-08-19T11:59:18.607243",
     "exception": false,
     "start_time": "2021-08-19T11:59:18.569534",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now that I loaded train and test datasets. Let me arrange columns by feature types. First of all, I am going to designate numerics columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7e168cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-19T11:59:18.688009Z",
     "iopub.status.busy": "2021-08-19T11:59:18.687217Z",
     "iopub.status.idle": "2021-08-19T11:59:18.690621Z",
     "shell.execute_reply": "2021-08-19T11:59:18.690020Z",
     "shell.execute_reply.started": "2021-08-18T18:21:08.549102Z"
    },
    "papermill": {
     "duration": 0.046226,
     "end_time": "2021-08-19T11:59:18.690790",
     "exception": false,
     "start_time": "2021-08-19T11:59:18.644564",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Setting numeric columns\n",
    "feature_columns = []\n",
    "# numeric columns\n",
    "for header in ['age', 'n_siblings_spouses', 'parch', 'fare']:\n",
    "    feature_columns.append(feature_column.numeric_column(header))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae95084",
   "metadata": {
    "papermill": {
     "duration": 0.036655,
     "end_time": "2021-08-19T11:59:18.764723",
     "exception": false,
     "start_time": "2021-08-19T11:59:18.728068",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "If you want, you can bin age into a bucket. First, let's take a look statistics of age column. To do this, I am going to use Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "009a79c4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-19T11:59:18.844484Z",
     "iopub.status.busy": "2021-08-19T11:59:18.843490Z",
     "iopub.status.idle": "2021-08-19T11:59:18.879602Z",
     "shell.execute_reply": "2021-08-19T11:59:18.879072Z",
     "shell.execute_reply.started": "2021-08-18T18:21:08.561143Z"
    },
    "papermill": {
     "duration": 0.077858,
     "end_time": "2021-08-19T11:59:18.879763",
     "exception": false,
     "start_time": "2021-08-19T11:59:18.801905",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>age</th>\n",
       "      <th>n_siblings_spouses</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>627.000000</td>\n",
       "      <td>627.000000</td>\n",
       "      <td>627.000000</td>\n",
       "      <td>627.000000</td>\n",
       "      <td>627.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.387560</td>\n",
       "      <td>29.631308</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.379585</td>\n",
       "      <td>34.385399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.487582</td>\n",
       "      <td>12.511818</td>\n",
       "      <td>1.151090</td>\n",
       "      <td>0.792999</td>\n",
       "      <td>54.597730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.895800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.045800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.387500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         survived         age  n_siblings_spouses       parch        fare\n",
       "count  627.000000  627.000000          627.000000  627.000000  627.000000\n",
       "mean     0.387560   29.631308            0.545455    0.379585   34.385399\n",
       "std      0.487582   12.511818            1.151090    0.792999   54.597730\n",
       "min      0.000000    0.750000            0.000000    0.000000    0.000000\n",
       "25%      0.000000   23.000000            0.000000    0.000000    7.895800\n",
       "50%      0.000000   28.000000            0.000000    0.000000   15.045800\n",
       "75%      1.000000   35.000000            1.000000    0.000000   31.387500\n",
       "max      1.000000   80.000000            8.000000    5.000000  512.329200"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic_df = pd.read_csv(train_file_path, header='infer')\n",
    "titanic_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8703c3d",
   "metadata": {
    "papermill": {
     "duration": 0.037387,
     "end_time": "2021-08-19T11:59:18.956139",
     "exception": false,
     "start_time": "2021-08-19T11:59:18.918752",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let me try three bin boundaries for age : 23, 28, and 35."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b41530a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-19T11:59:19.039986Z",
     "iopub.status.busy": "2021-08-19T11:59:19.039305Z",
     "iopub.status.idle": "2021-08-19T11:59:19.041484Z",
     "shell.execute_reply": "2021-08-19T11:59:19.041946Z",
     "shell.execute_reply.started": "2021-08-18T18:21:08.613871Z"
    },
    "papermill": {
     "duration": 0.047335,
     "end_time": "2021-08-19T11:59:19.042134",
     "exception": false,
     "start_time": "2021-08-19T11:59:18.994799",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Bucketizing age columns\n",
    "age = feature_column.numeric_column('age')\n",
    "age_buckets = feature_column.bucketized_column(age, boundaries=[23, 28, 35])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b68ef4e",
   "metadata": {
    "papermill": {
     "duration": 0.037901,
     "end_time": "2021-08-19T11:59:19.118997",
     "exception": false,
     "start_time": "2021-08-19T11:59:19.081096",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "To use one-hot encode, I am going to see the distinct values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8583d7dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-19T11:59:19.198659Z",
     "iopub.status.busy": "2021-08-19T11:59:19.197985Z",
     "iopub.status.idle": "2021-08-19T11:59:19.211328Z",
     "shell.execute_reply": "2021-08-19T11:59:19.211888Z",
     "shell.execute_reply.started": "2021-08-18T18:21:08.621833Z"
    },
    "papermill": {
     "duration": 0.054956,
     "end_time": "2021-08-19T11:59:19.212062",
     "exception": false,
     "start_time": "2021-08-19T11:59:19.157106",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sex : ['male' 'female']\n",
      "class : ['Third' 'First' 'Second']\n",
      "deck : ['unknown' 'C' 'G' 'A' 'B' 'D' 'F' 'E']\n",
      "embark_town : ['Southampton' 'Cherbourg' 'Queenstown' 'unknown']\n",
      "alone : ['n' 'y']\n"
     ]
    }
   ],
   "source": [
    "#Deteriming categorical columns\n",
    "h = {}\n",
    "for col in titanic_df:\n",
    "    if col in ['sex', 'class', 'deck', 'embark_town', 'alone']:\n",
    "        print(col, ':', titanic_df[col].unique())\n",
    "        h[col] = titanic_df[col].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce1faf9d",
   "metadata": {
    "papermill": {
     "duration": 0.037619,
     "end_time": "2021-08-19T11:59:19.288264",
     "exception": false,
     "start_time": "2021-08-19T11:59:19.250645",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's use categorical_column_with_vocabulary_list since inputs are in string format. Let me keep track of these unique values using h variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d785402",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-19T11:59:19.373626Z",
     "iopub.status.busy": "2021-08-19T11:59:19.372811Z",
     "iopub.status.idle": "2021-08-19T11:59:19.376134Z",
     "shell.execute_reply": "2021-08-19T11:59:19.375626Z",
     "shell.execute_reply.started": "2021-08-18T18:21:08.650524Z"
    },
    "papermill": {
     "duration": 0.049123,
     "end_time": "2021-08-19T11:59:19.376275",
     "exception": false,
     "start_time": "2021-08-19T11:59:19.327152",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Converting categorical columns and encoding unique categorical values\n",
    "sex_type = feature_column.categorical_column_with_vocabulary_list(\n",
    "      'Type', h.get('sex').tolist())\n",
    "sex_type_one_hot = feature_column.indicator_column(sex_type)\n",
    "\n",
    "class_type = feature_column.categorical_column_with_vocabulary_list(\n",
    "      'Type', h.get('class').tolist())\n",
    "class_type_one_hot = feature_column.indicator_column(class_type)\n",
    "\n",
    "deck_type = feature_column.categorical_column_with_vocabulary_list(\n",
    "      'Type', h.get('deck').tolist())\n",
    "deck_type_one_hot = feature_column.indicator_column(deck_type)\n",
    "\n",
    "embark_town_type = feature_column.categorical_column_with_vocabulary_list(\n",
    "      'Type', h.get('embark_town').tolist())\n",
    "embark_town_type_one_hot = feature_column.indicator_column(embark_town_type)\n",
    "\n",
    "alone_type = feature_column.categorical_column_with_vocabulary_list(\n",
    "      'Type', h.get('alone').tolist())\n",
    "alone_one_hot = feature_column.indicator_column(alone_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27da5f37",
   "metadata": {
    "papermill": {
     "duration": 0.038785,
     "end_time": "2021-08-19T11:59:19.453508",
     "exception": false,
     "start_time": "2021-08-19T11:59:19.414723",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\"deck\" column has eight unique values so I am going to embed this column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9e079c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-19T11:59:19.537036Z",
     "iopub.status.busy": "2021-08-19T11:59:19.536323Z",
     "iopub.status.idle": "2021-08-19T11:59:19.539674Z",
     "shell.execute_reply": "2021-08-19T11:59:19.539073Z",
     "shell.execute_reply.started": "2021-08-18T18:21:08.661481Z"
    },
    "papermill": {
     "duration": 0.046582,
     "end_time": "2021-08-19T11:59:19.539822",
     "exception": false,
     "start_time": "2021-08-19T11:59:19.493240",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Embeding the \"deck\" column and reducing its dimension to 3.\n",
    "deck = feature_column.categorical_column_with_vocabulary_list(\n",
    "      'deck', titanic_df.deck.unique())\n",
    "deck_embedding = feature_column.embedding_column(deck, dimension=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e8cec3",
   "metadata": {
    "papermill": {
     "duration": 0.037543,
     "end_time": "2021-08-19T11:59:19.615287",
     "exception": false,
     "start_time": "2021-08-19T11:59:19.577744",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's reduce the dimensions of class columns using a hashed feature column. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71c7787d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-19T11:59:19.696423Z",
     "iopub.status.busy": "2021-08-19T11:59:19.695438Z",
     "iopub.status.idle": "2021-08-19T11:59:19.698233Z",
     "shell.execute_reply": "2021-08-19T11:59:19.698766Z",
     "shell.execute_reply.started": "2021-08-18T18:21:08.678422Z"
    },
    "papermill": {
     "duration": 0.045758,
     "end_time": "2021-08-19T11:59:19.698939",
     "exception": false,
     "start_time": "2021-08-19T11:59:19.653181",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reducing class column\n",
    "class_hashed = feature_column.categorical_column_with_hash_bucket(\n",
    "      'class', hash_bucket_size=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cef8842",
   "metadata": {
    "papermill": {
     "duration": 0.037725,
     "end_time": "2021-08-19T11:59:19.774589",
     "exception": false,
     "start_time": "2021-08-19T11:59:19.736864",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "There may be interaction between passenger gender and cabin class. Let's encode those intercations using crossed_column() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e60ca8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-19T11:59:19.855262Z",
     "iopub.status.busy": "2021-08-19T11:59:19.854601Z",
     "iopub.status.idle": "2021-08-19T11:59:19.857759Z",
     "shell.execute_reply": "2021-08-19T11:59:19.857191Z",
     "shell.execute_reply.started": "2021-08-18T18:21:08.693441Z"
    },
    "papermill": {
     "duration": 0.045531,
     "end_time": "2021-08-19T11:59:19.857895",
     "exception": false,
     "start_time": "2021-08-19T11:59:19.812364",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cross_type_feature = feature_column.crossed_column(['sex', 'class'], hash_bucket_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79687285",
   "metadata": {
    "papermill": {
     "duration": 0.039225,
     "end_time": "2021-08-19T11:59:19.935436",
     "exception": false,
     "start_time": "2021-08-19T11:59:19.896211",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now that I am going to put together what I've done. Let's create a list to hold all the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4de345a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-19T11:59:20.022498Z",
     "iopub.status.busy": "2021-08-19T11:59:20.020547Z",
     "iopub.status.idle": "2021-08-19T11:59:20.025827Z",
     "shell.execute_reply": "2021-08-19T11:59:20.025293Z",
     "shell.execute_reply.started": "2021-08-18T18:21:08.708282Z"
    },
    "papermill": {
     "duration": 0.052445,
     "end_time": "2021-08-19T11:59:20.025972",
     "exception": false,
     "start_time": "2021-08-19T11:59:19.973527",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_columns = []\n",
    "\n",
    "# appending numeric columns\n",
    "for header in ['age', 'n_siblings_spouses', 'parch', 'fare']:\n",
    "    feature_columns.append(feature_column.numeric_column(header))\n",
    "    \n",
    "# appending bucketized columns\n",
    "age = feature_column.numeric_column('age')\n",
    "age_buckets = feature_column.bucketized_column(age, boundaries=[23, 28, 35])\n",
    "feature_columns.append(age_buckets)\n",
    "\n",
    "# appending categorical columns\n",
    "indicator_column_names = ['sex', 'class', 'deck', 'embark_town', 'alone']\n",
    "for col_name in indicator_column_names:\n",
    "    categorical_column = feature_column.categorical_column_with_vocabulary_list(\n",
    "        col_name, titanic_df[col_name].unique())\n",
    "    indicator_column = feature_column.indicator_column(categorical_column)\n",
    "    feature_columns.append(indicator_column)\n",
    "    \n",
    "# appending embedding columns\n",
    "deck = feature_column.categorical_column_with_vocabulary_list(\n",
    "      'deck', titanic_df.deck.unique())\n",
    "deck_embedding = feature_column.embedding_column(deck, dimension=3)\n",
    "feature_columns.append(deck_embedding)\n",
    "\n",
    "# appending crossed columns\n",
    "feature_columns.append(feature_column.indicator_column(cross_type_feature))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e7633d",
   "metadata": {
    "papermill": {
     "duration": 0.037685,
     "end_time": "2021-08-19T11:59:20.101752",
     "exception": false,
     "start_time": "2021-08-19T11:59:20.064067",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Now I am going to create a feature layer. This layer will serve as the first (input) layer in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7d1062b7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-19T11:59:20.184812Z",
     "iopub.status.busy": "2021-08-19T11:59:20.184117Z",
     "iopub.status.idle": "2021-08-19T11:59:20.198548Z",
     "shell.execute_reply": "2021-08-19T11:59:20.197973Z",
     "shell.execute_reply.started": "2021-08-18T18:21:08.725064Z"
    },
    "papermill": {
     "duration": 0.058028,
     "end_time": "2021-08-19T11:59:20.198698",
     "exception": false,
     "start_time": "2021-08-19T11:59:20.140670",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545256ab",
   "metadata": {
    "papermill": {
     "duration": 0.037699,
     "end_time": "2021-08-19T11:59:20.274465",
     "exception": false,
     "start_time": "2021-08-19T11:59:20.236766",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let me split test_df into validation and test datasets. Hyperparameters are fine tuned using validation dataset and model is evaluated using test dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "848185dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-19T11:59:20.359290Z",
     "iopub.status.busy": "2021-08-19T11:59:20.358246Z",
     "iopub.status.idle": "2021-08-19T11:59:20.361775Z",
     "shell.execute_reply": "2021-08-19T11:59:20.361173Z",
     "shell.execute_reply.started": "2021-08-18T18:21:08.755438Z"
    },
    "papermill": {
     "duration": 0.049778,
     "end_time": "2021-08-19T11:59:20.361919",
     "exception": false,
     "start_time": "2021-08-19T11:59:20.312141",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "val_df, test_df = train_test_split(test_df, test_size=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d5b1a7",
   "metadata": {
    "papermill": {
     "duration": 0.038264,
     "end_time": "2021-08-19T11:59:20.438510",
     "exception": false,
     "start_time": "2021-08-19T11:59:20.400246",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let me specify target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aab70b26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-19T11:59:20.537785Z",
     "iopub.status.busy": "2021-08-19T11:59:20.536996Z",
     "iopub.status.idle": "2021-08-19T11:59:20.543622Z",
     "shell.execute_reply": "2021-08-19T11:59:20.542799Z",
     "shell.execute_reply.started": "2021-08-18T18:21:08.765475Z"
    },
    "papermill": {
     "duration": 0.064747,
     "end_time": "2021-08-19T11:59:20.543833",
     "exception": false,
     "start_time": "2021-08-19T11:59:20.479086",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "labels = train_df.pop(\"survived\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40989be0",
   "metadata": {
    "papermill": {
     "duration": 0.03765,
     "end_time": "2021-08-19T11:59:20.628680",
     "exception": false,
     "start_time": "2021-08-19T11:59:20.591030",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "To stream the data into the training process with the dataset, I am going to create a function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d9c6cd4e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-19T11:59:20.707324Z",
     "iopub.status.busy": "2021-08-19T11:59:20.706717Z",
     "iopub.status.idle": "2021-08-19T11:59:20.711978Z",
     "shell.execute_reply": "2021-08-19T11:59:20.712532Z",
     "shell.execute_reply.started": "2021-08-18T18:21:08.779844Z"
    },
    "papermill": {
     "duration": 0.04615,
     "end_time": "2021-08-19T11:59:20.712705",
     "exception": false,
     "start_time": "2021-08-19T11:59:20.666555",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pandas_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "    dataframe = dataframe.copy()\n",
    "    labels = dataframe.pop('survived')\n",
    "    # To transform the DataFrame into a key-value pair. \n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "    # To shuffle and batch\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d7f288",
   "metadata": {
    "papermill": {
     "duration": 0.037722,
     "end_time": "2021-08-19T11:59:20.789173",
     "exception": false,
     "start_time": "2021-08-19T11:59:20.751451",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Appliying this function to both validation and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff65df6b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-19T11:59:20.867819Z",
     "iopub.status.busy": "2021-08-19T11:59:20.867107Z",
     "iopub.status.idle": "2021-08-19T11:59:20.901768Z",
     "shell.execute_reply": "2021-08-19T11:59:20.902308Z",
     "shell.execute_reply.started": "2021-08-18T18:22:38.287851Z"
    },
    "papermill": {
     "duration": 0.075673,
     "end_time": "2021-08-19T11:59:20.902507",
     "exception": false,
     "start_time": "2021-08-19T11:59:20.826834",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size=32\n",
    "val_ds = pandas_to_dataset(val_df, shuffle=False, batch_size=batch_size)\n",
    "test_ds = pandas_to_dataset(test_df, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e9d6ee",
   "metadata": {
    "papermill": {
     "duration": 0.038702,
     "end_time": "2021-08-19T11:59:20.978933",
     "exception": false,
     "start_time": "2021-08-19T11:59:20.940231",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bf31cf1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-19T11:59:21.058967Z",
     "iopub.status.busy": "2021-08-19T11:59:21.058319Z",
     "iopub.status.idle": "2021-08-19T11:59:21.076886Z",
     "shell.execute_reply": "2021-08-19T11:59:21.077419Z",
     "shell.execute_reply.started": "2021-08-18T18:22:41.465248Z"
    },
    "papermill": {
     "duration": 0.060163,
     "end_time": "2021-08-19T11:59:21.077631",
     "exception": false,
     "start_time": "2021-08-19T11:59:21.017468",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "  feature_layer,\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dense(128, activation='relu'),\n",
    "  layers.Dropout(.1),\n",
    "  layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "490db1bb",
   "metadata": {
    "papermill": {
     "duration": 0.038485,
     "end_time": "2021-08-19T11:59:21.154300",
     "exception": false,
     "start_time": "2021-08-19T11:59:21.115815",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Take a look summary of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50df5d57",
   "metadata": {
    "papermill": {
     "duration": 0.040491,
     "end_time": "2021-08-19T11:59:21.233413",
     "exception": false,
     "start_time": "2021-08-19T11:59:21.192922",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Compiling the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d1a6eb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-19T11:59:21.319406Z",
     "iopub.status.busy": "2021-08-19T11:59:21.318733Z",
     "iopub.status.idle": "2021-08-19T11:59:21.503870Z",
     "shell.execute_reply": "2021-08-19T11:59:21.503179Z",
     "shell.execute_reply.started": "2021-08-18T18:23:19.236495Z"
    },
    "papermill": {
     "duration": 0.232554,
     "end_time": "2021-08-19T11:59:21.504015",
     "exception": false,
     "start_time": "2021-08-19T11:59:21.271461",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', \n",
    "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc0c612",
   "metadata": {
    "papermill": {
     "duration": 0.037981,
     "end_time": "2021-08-19T11:59:21.580570",
     "exception": false,
     "start_time": "2021-08-19T11:59:21.542589",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "436cf79e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-19T11:59:21.661824Z",
     "iopub.status.busy": "2021-08-19T11:59:21.661065Z",
     "iopub.status.idle": "2021-08-19T11:59:28.983064Z",
     "shell.execute_reply": "2021-08-19T11:59:28.982530Z",
     "shell.execute_reply.started": "2021-08-18T18:23:22.885044Z"
    },
    "papermill": {
     "duration": 7.364586,
     "end_time": "2021-08-19T11:59:28.983207",
     "exception": false,
     "start_time": "2021-08-19T11:59:21.618621",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "209/209 [==============================] - 3s 6ms/step - loss: 1.0256 - accuracy: 0.5975 - val_loss: 0.8080 - val_accuracy: 0.6772\n",
      "Epoch 2/10\n",
      "209/209 [==============================] - 0s 2ms/step - loss: 0.7751 - accuracy: 0.6451 - val_loss: 0.5000 - val_accuracy: 0.7215\n",
      "Epoch 3/10\n",
      "209/209 [==============================] - 1s 2ms/step - loss: 0.6317 - accuracy: 0.6941 - val_loss: 0.4800 - val_accuracy: 0.7089\n",
      "Epoch 4/10\n",
      "209/209 [==============================] - 0s 2ms/step - loss: 0.5270 - accuracy: 0.7525 - val_loss: 0.4562 - val_accuracy: 0.7342\n",
      "Epoch 5/10\n",
      "209/209 [==============================] - 0s 2ms/step - loss: 0.5803 - accuracy: 0.7735 - val_loss: 0.6480 - val_accuracy: 0.7215\n",
      "Epoch 6/10\n",
      "209/209 [==============================] - 0s 2ms/step - loss: 0.5398 - accuracy: 0.7385 - val_loss: 0.5039 - val_accuracy: 0.7595\n",
      "Epoch 7/10\n",
      "209/209 [==============================] - 0s 2ms/step - loss: 0.5219 - accuracy: 0.7527 - val_loss: 0.4476 - val_accuracy: 0.7848\n",
      "Epoch 8/10\n",
      "209/209 [==============================] - 0s 2ms/step - loss: 0.4592 - accuracy: 0.8005 - val_loss: 0.4463 - val_accuracy: 0.7785\n",
      "Epoch 9/10\n",
      "209/209 [==============================] - 0s 2ms/step - loss: 0.5535 - accuracy: 0.7810 - val_loss: 0.4626 - val_accuracy: 0.7785\n",
      "Epoch 10/10\n",
      "209/209 [==============================] - 0s 2ms/step - loss: 0.4951 - accuracy: 0.7784 - val_loss: 0.4578 - val_accuracy: 0.7848\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fa0aff829d0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_ds,\n",
    "          validation_data=val_ds,\n",
    "          epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c31b59",
   "metadata": {
    "papermill": {
     "duration": 0.070573,
     "end_time": "2021-08-19T11:59:29.125105",
     "exception": false,
     "start_time": "2021-08-19T11:59:29.054532",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "That is all. In this tutorail, I am going to showed how to prepare tabular dataset to analyze and deal with multiple data types. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 30.913343,
   "end_time": "2021-08-19T11:59:31.214157",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-08-19T11:59:00.300814",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
