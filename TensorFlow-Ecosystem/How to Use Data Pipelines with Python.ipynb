{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "907ea832",
   "metadata": {},
   "source": [
    "# How to Use Data Pipelines withÂ Python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fb8a66",
   "metadata": {},
   "source": [
    "An important steps of your workflow is data ingestion. Before building the model, the data have to be the correct format. To do this, several steps called data pipeline perform. In this chapter, I am going to show how to handle file structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1e29ce",
   "metadata": {},
   "source": [
    "## What is Data Pipeline?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c88195",
   "metadata": {},
   "source": [
    "To analyze text data, you need to organize the directory structure correctly. For example, if you want to make text classification, you have to organize your training texts into positives and negatives. When you'll classify dataset as positive and negative your directory structure can be as follows:\n",
    "pos\n",
    "    p1.txt\n",
    "    p2.txt\n",
    "neg \n",
    "    n1.txt\n",
    "    n2.txt\n",
    "Let's thing of the Internet Movie Database (IMDB) dataset and classify movie reviews as positive and negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e7bcfe",
   "metadata": {},
   "source": [
    "### Downloading Text Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867fdaf2",
   "metadata": {},
   "source": [
    "First of all, let me import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25b8b63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import string\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3fe963",
   "metadata": {},
   "source": [
    "Now that, I am going to load IMDb dataset. If you want to download directly you can use get_file() method. To do this, let me create url variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b05cbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cfd4df9",
   "metadata": {},
   "source": [
    "and then let me use get_file() method as follows : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2fb9da1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = tf.keras.utils.get_file(\"aclImdb_v1.tar.gz\", \n",
    "                             url,untar=True, \n",
    "                             cache_dir='.',\n",
    "                             cache_subdir='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09df520e",
   "metadata": {},
   "source": [
    "So dataset downloaded and a directory is created called aclImdb in the current directory. I am going to create a variable, which represent this file path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72371ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.path.join(os.path.dirname(ds), 'aclImdb')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9d27ea",
   "metadata": {},
   "source": [
    "Let's take a look inside of directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7b160cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['labeledBow.feat',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'unsup',\n",
       " 'unsupBow.feat',\n",
       " 'urls_neg.txt',\n",
       " 'urls_pos.txt',\n",
       " 'urls_unsup.txt']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dir = os.path.join(data_dir, 'train')\n",
    "os.listdir(train_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a155f19a",
   "metadata": {},
   "source": [
    "There is one directory called unsup. I don't need unsup directory. I want to remove this directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4317e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "unused_dir = os.path.join(train_dir, 'unsup')\n",
    "shutil.rmtree(unused_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e01ed0",
   "metadata": {},
   "source": [
    "Let me take a look train_dir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0bb5ab97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['labeledBow.feat',\n",
       " 'neg',\n",
       " 'pos',\n",
       " 'unsupBow.feat',\n",
       " 'urls_neg.txt',\n",
       " 'urls_pos.txt',\n",
       " 'urls_unsup.txt']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(train_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad454c0",
   "metadata": {},
   "source": [
    "As you can see, unsup is removed from directory of data. Make sure you have only directory names are used as labels. \n",
    "Let me see the content in the training directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c6a7f16f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is Windows\n",
      " Volume Serial Number is A065-837B\n",
      "\n",
      " Directory of C:\\TiAk\\My-Notebooks\\TensorFlow 2 Pocket Reference\\Chapter-4\\aclImdb\\train\n",
      "\n",
      "26.08.2021  18:16    <DIR>          .\n",
      "26.08.2021  18:16    <DIR>          ..\n",
      "12.04.2011  20:17        21.021.197 labeledBow.feat\n",
      "12.04.2011  12:47    <DIR>          neg\n",
      "12.04.2011  12:47    <DIR>          pos\n",
      "12.04.2011  20:22        41.348.699 unsupBow.feat\n",
      "12.04.2011  12:48           612.500 urls_neg.txt\n",
      "12.04.2011  12:48           612.500 urls_pos.txt\n",
      "12.04.2011  12:47         2.450.000 urls_unsup.txt\n",
      "               5 File(s)     66.044.896 bytes\n",
      "               4 Dir(s)  119.645.503.488 bytes free\n"
     ]
    }
   ],
   "source": [
    "ls aclImdb\\train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ffcc244",
   "metadata": {},
   "source": [
    "As you can see there are pos and neg directories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cff3f106",
   "metadata": {},
   "source": [
    "### Creating the Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff3c3c7",
   "metadata": {},
   "source": [
    "Now that I am going to create pipeline. First, let me create a few variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b025e15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1024\n",
    "seed = 123"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55681cb",
   "metadata": {},
   "source": [
    "I am going to want to generates a tf.data.Dataset from text files in a directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "076bdb3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 20000 files for training.\n"
     ]
    }
   ],
   "source": [
    "train_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    'aclImdb/train', \n",
    "    batch_size=batch_size, \n",
    "    validation_split=0.2,\n",
    "    subset='training', \n",
    "    seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838321d5",
   "metadata": {},
   "source": [
    "So I created train datasets. To fine tune hyperparameter, I'll use validation dataset. Let's create validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f4b2b5df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25000 files belonging to 2 classes.\n",
      "Using 5000 files for validation.\n"
     ]
    }
   ],
   "source": [
    "val_ds = tf.keras.preprocessing.text_dataset_from_directory(\n",
    "    'aclImdb/train', \n",
    "    batch_size=batch_size, \n",
    "    validation_split=0.2,\n",
    "    subset='validation', \n",
    "    seed=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75a496b",
   "metadata": {},
   "source": [
    "### Inspecting the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad7455f2",
   "metadata": {},
   "source": [
    "I am going to inspect at the content of these files. Let's select randomly five rows in first batch and print out them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c02e76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 b'Clint Eastwood has definitely produced better movies than this, but this one does not embarrass him. Dirty Harry catches everyone\\'s attention and unless one wants to watch romance, there is no reason why you won\\'t like him. He is cool because he is dirty, is great because he kills without much thinking, is perfect because he gets the bullet right through your heart and a hero because he doesn\\'t care.<br /><br />From what I have seen in movies in which Eastwood acts, the character of the lead role always captivates the audience. In White Hunter Black heart, he is the crazy director, in \"in the Line of Fire\" he is the \"Old \\'un\" while here is the \"almost\" jobless with his job, that is to say he makes work for himself, doesn\\'t care one damn about his superiors who practically send him out for a vacation.<br /><br />Based on a rape victim, this movie is promising for all the \"no non-sense\" movie watchers. The movie has nothing that goes away from he central plot. However, what makes it slightly inferior to the better movies of Eastwood is that though the character of the lead role is captivating the plot is not, as it is far too obvious from the beginning. It is not a movie that is going to make you sit at a place without moving. Also, there are too many people far dirtier than Dirty harry.'\n",
      "1 b'Some genre films need to be dressed up. This one was an exception. Taken on its own merit, it\\'s a dressed down version of the horror genre film. With minimal special effects, it manages to be a psychological study of sorts, with a simple yet existential theme - who gets hit by the bus, and why her? It\\'s not a great film, yet because there is little contrived about it, the story works. Subtle, and all about the interactions of the characters. Actually, there is one contrivance in the opening scenes, but it may have been placed there to simply set the tone for what\\'s to come. I very much appreciate the balance of male and female energy, and would not recommend this story to anyone interested in more than people reacting to a physical and psychological challenge. You will enjoy the film if you have some empathy, value the need for a bit of adventure in your life, and wonder \"What would I do in this situation?\"'\n",
      "0 b'The buzz for this film has always been about the fabulous graphics that make Kevin Bacon disappear. Sadly, they stopped there. They should have continued to make the script disappear, then the silly set, and finally every visible element of this film. Because, there\\'s nothing else there to show.<br /><br />Gary Thompson and Andrew Marlowe are listed as the writing credits for this film. I don\\'t really think they exist. I think they bought this script at \"Scripts-R-Us\", where you buy a standard blank \"Monster Movie\" script and just fill in the blanks. There\\'s a monster stalking us. Let\\'s split up. (They actually \"let\\'s split up\" in this movie). Hit Alien/Giant-bug/Monster/Invisible-man with crowbar. Not dead yet. Burn Huge-rabbit/Shark/Invisible-man in unsurvivable fire. Not dead yet. You know, the standard stuff. Even the minimum number of elements that were specific to an invisible man movie (IR glasses, spraying with something like paint) were handled badly. <br /><br />What is sad is that there were lots of possibilities for this to be a fascinating movie. They psychological issues for the subject, the deterioration of the mind due to the process, treating an invisible subject, and many other ideas were touched on for usually less than 2 seconds and would have been far more interesting. Had there been any desire to save Kevin Bacon in the end, it would have been a much better movie. All in all, it stunk.<br /><br />I would mention some of the incredibly stupid elements of the ending of the movie, but I don\\'t want to do any spoilers. Suffice it to say that these characters are so stupid they don\\'t think about pulling the plug on a machine rather than...'\n",
      "1 b\"An entertaining kung fu film, with acting, plot and fight scenes a cut above the average chop socky. All of the cast are likeable characters and skilled martial artists. Alexander Fu-Sheng's proto-Jackie Chan comedy antics are fun to watch, and his austere companion shows particularly impressive skills. For me, the film's only glaring flaw is the size of the cast -- at times, things get a little confused as the film chops and changes between various subplots, and some of the characters are not as fully fleshed-out as one might wish.<br /><br />But a kung fu film should be judged first and foremost on the quality of the action, and Shaolin Temple definitely delivers on that count. The film climaxes with a high-bodycount battle that allows each character to show off his skills against a worthy opponent.<br /><br />Overall, Shaolin Temple is an enjoyable low-budget kung fu movie. Not up to the quality of a good Jet Li film, but definitely worth a look for fans of the genre. My rating: 8/10.<br /><br />Misc notes: The 1987 Warner Home Video release I saw was (predictably) poorly dubbed, and lacked full cast & crew credits.\"\n",
      "1 b\"i got to see the whole movie last night and i found it very exciting.it was at least,not like the teen-slasher movies that pop out every now and then.the search for the killer and the 'partner' relationship between the hero&the so-called bad guy was parts i liked about the movie.also,i remember once being on the edge of my seat during a specific scene in the movie.i mean it's exciting.maybe some time later,i might watch the movie again...\"\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "idx = random.sample(range(1, batch_size), 5)\n",
    "for text_batch, label_batch in train_ds.take(1):\n",
    "    for i in idx:\n",
    "        print(label_batch[i].numpy(), text_batch.numpy()[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb8acb5",
   "metadata": {},
   "source": [
    "In this section, I showed how to deal with text datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad706ea1",
   "metadata": {},
   "source": [
    "## The Data Pipeline for ImageÂ Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8d6e61",
   "metadata": {},
   "source": [
    "In this section, I am gonig to show how to deal with data ingestion pipeline for image data. \n",
    "\n",
    "Sometimes, you have images in the same file. This file includes two columns : one with all the filenames and one with the labels. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca112eeb",
   "metadata": {},
   "source": [
    "### Downloading Images "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05039df3",
   "metadata": {},
   "source": [
    "First, I am going to download [flower dataset](https://data.mendeley.com/public-files/datasets/jxmfrvhpyz/files/283004ff-e529-4c3c-a1ee-4fb90024dc94/file_downloaded)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb155d9",
   "metadata": {},
   "source": [
    "I am going to build a data pipeline to feed these images into an image classification model for training. Let me stream these images into the training process with ImageDataGenerator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c93c78",
   "metadata": {},
   "source": [
    "### Creating the Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb778500",
   "metadata": {},
   "source": [
    "Let me import libraries, which will use in this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c0ab61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d856c950",
   "metadata": {},
   "source": [
    "Note that dataset has a label file. I am going to see its contents using pandas library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56815b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf=pd.read_csv('flower_photos/all_labels.csv',dtype=str)\n",
    "traindf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8797a0fd",
   "metadata": {},
   "source": [
    "### Preprocessing the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe89f49b",
   "metadata": {},
   "source": [
    "Now that I am goning to create some hyperparameters, which will be used later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed3921a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = 'flower_photos/flowers'\n",
    "IMAGE_SIZE = (224, 224)\n",
    "TRAINING_DATA_DIR = str(data_root)\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60b38b3",
   "metadata": {},
   "source": [
    "I am going to normalize dataset and reserve 20% of the images for validation dataset. Let me use a dictionary structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaeb00e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen_kwargs = dict(\n",
    "    rescale=1./255, \n",
    "    validation_split=.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460c00ea",
   "metadata": {},
   "source": [
    "To build the model, I am going to use the prebuilted ResNet model. The ResNet model expects images to have pixel dimensions of 224\\*224 and I need to determine the batch size and resample algorithm as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a5014c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataflow_kwargs = dict(\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    interpolation=\"bilinear\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f262557c",
   "metadata": {},
   "source": [
    "To train the images, I am going to define generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808a3fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    **datagen_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6790ef",
   "metadata": {},
   "source": [
    "Let me create a data flow pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d85a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator=train_datagen.flow_from_dataframe(\n",
    "    dataframe=traindf,\n",
    "    directory=data_root,\n",
    "    x_col=\"file_name\",\n",
    "    y_col=\"label\",\n",
    "    subset=\"training\",\n",
    "    seed=10,\n",
    "    shuffle=True,\n",
    "    class_mode=\"categorical\",\n",
    "    **dataflow_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb415517",
   "metadata": {},
   "source": [
    "### Inspecting the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f86519",
   "metadata": {},
   "source": [
    "Let me show images in dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb817e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batch, label_batch = next(iter(train_generator))\n",
    "fig, axes = plt.subplots(8, 4, figsize=(20, 40))\n",
    "axes = axes.flatten()\n",
    "for img, lbl, ax in zip(image_batch, label_batch, axes):\n",
    "    ax.imshow(img)\n",
    "    label_ = np.argmax(lbl)\n",
    "    label = idx_labels[label_]\n",
    "    ax.set_title(label)\n",
    "    ax.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca2cfb7",
   "metadata": {},
   "source": [
    "So data ingestion pipeline is ready to use. Let's train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299de2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer( input_shape=IMAGE_SIZE + (3,)), \n",
    "    hub.KerasLayer( \"https://tfhub.dev/tensorflow/resnet_50/feature_vector/1\", \n",
    "                   trainable=False),\n",
    "    tf.keras.layers.Dense(5, \n",
    "                          activation='softmax', \n",
    "                          name = 'custom_class')])\n",
    "mdl.build([None, 224, 224, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dae9bce",
   "metadata": {},
   "source": [
    "Let's compile the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29373a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl.compile(\n",
    "  optimizer=tf.keras.optimizers.SGD(lr=0.005, momentum=0.9),\n",
    "  loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.1),\n",
    "  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa963b47",
   "metadata": {},
   "source": [
    "I am going to train the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b28ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = train_generator.samples // train_generator.batch_size\n",
    "validation_steps = valid_generator.samples // valid_generator.batch_size\n",
    "\n",
    "mdl.fit(\n",
    "    train_generator,\n",
    "    epochs=13, steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=valid_generator,\n",
    "    validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17c8913",
   "metadata": {},
   "source": [
    "That's it. As you can see, the training image generator and validation image generator are passed into training process. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca532a6c",
   "metadata": {},
   "source": [
    "In this tutorial, I showed how to use the data ingestion pipeline. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "726223df",
   "metadata": {},
   "source": [
    "## Data Pipeline for NumPy ArrayÂ Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c8269e1",
   "metadata": {},
   "source": [
    "In this tutorial, I am going to show how to create a data pipeline using a NumPy array. To do this, I am going to use from_tensor_slices method. \n",
    "\n",
    "Let's use Fashion MNIST dataset, which consists of 10 types of garments in grayscale. The images are represented using a NumPy structure instead of a typical image format, such as JPEG or PNG. \n",
    "\n",
    "You can easily download using tf.Keras API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfe855e",
   "metadata": {},
   "source": [
    "### Loading the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5014ea92",
   "metadata": {},
   "source": [
    "First of all, let me import libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a168e558",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bec8073",
   "metadata": {},
   "source": [
    "Let's load datasets using the load_data function in the tf.keras API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea45cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcecb37",
   "metadata": {},
   "source": [
    "Let me take a look the structure of datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f9736d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(train_images), type(train_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507268f9",
   "metadata": {},
   "source": [
    "As you can see, the structures of datasets are NumPy arrays. Now that  am going to look at the shapes of datasets using shape command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449800dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_images.shape, train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b0aca9c",
   "metadata": {},
   "source": [
    "### Inspecting the NumPy Array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa6ff66",
   "metadata": {},
   "source": [
    "To visualize a NumPy array as a color scale, I am going to use matplotlib library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3f7e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(train_images[5])\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1288a7",
   "metadata": {},
   "source": [
    "### Preprocessing the Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fc334e",
   "metadata": {},
   "source": [
    "The images consist of pixel values between 0 and 255. To built faster the model and to get better accuracy, I am going to normalize the pixel values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4efef3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images/255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca1373f",
   "metadata": {},
   "source": [
    "I am going to build a streaming pipeline using from_tensor_slices method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd682a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f34977",
   "metadata": {},
   "source": [
    "Let me split this dataset into training and validation sets.The hyperparameters are fine tuned with the validation dataset and the model is built with train dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7835f6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHUFFLE_BUFFER_SIZE = 10000\n",
    "TRAIN_BATCH_SIZE = 50\n",
    "VALIDATION_BATCH_SIZE = 10000\n",
    "\n",
    "# To shuffle train dataset\n",
    "validation_ds = train_dataset.shuffle(\n",
    "    SHUFFLE_BUFFER_SIZE).take(\n",
    "    VALIDATION_SAMPLE_SIZE).batch(VALIDATION_BATCH_SIZE)\n",
    "train_ds = train_dataset.skip(\n",
    "    VALIDATION_BATCH_SIZE).batch(\n",
    "    TRAIN_BATCH_SIZE).repeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d109fa4d",
   "metadata": {},
   "source": [
    "### Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50cfea8",
   "metadata": {},
   "source": [
    "The datasets is ready to build the model. To train the model, I am going to use Sequential model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648347fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(30, activation='relu'),\n",
    "    tf.keras.layers.Dense(10)\n",
    "    \n",
    "# Compiling the model\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(),\n",
    "  loss=tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "  from_logits=True),\n",
    "  metrics=['sparse_categorical_accuracy'])\n",
    "    \n",
    "#Trainging the model\n",
    "model.fit(\n",
    "    train_ds,\n",
    "    epochs=13, steps_per_epoch=steps_per_epoch,\n",
    "    validation_data=validation_ds,\n",
    "    validation_steps=validation_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc76ce6",
   "metadata": {},
   "source": [
    "That's it. So train_ds and validation_ds were passed into training process. In this section, I showed how to create a data pipeline using from_tensor_slices for dataset which consists of NumPy array."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
