{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# <b><div style='padding:15px;background-color:#850E35;color:white;border-radius:2px;font-size:110%;text-align: center'> Text Generation with TensorFlow</div></b>",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "In this notebook, we'll walk you through how to generate text using a character RNN model. Here are the topics we'll cover:\n- Imports the required libraries\n- Downloads the Shakespeare dataset\n- Preprocesses the text data\n- Defines a model architecture\n- Compiles the model\n- Trains the model\n- Generates text using the trained model",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "# <b><div style='padding:15px;background-color:#850E35;color:white;border-radius:2px;font-size:110%;text-align: center'>1. Data Loading</div></b>",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "In this section, we begin by importing the TensorFlow library and proceed to download a dataset containing Shakespearean text from a remote URL. The downloaded text is stored in a variable called text, and we display the first 100 characters of the text for initial exploration.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Let's import TensorFlow library:\n",
    "import tensorflow as tf"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-13T17:05:58.059954Z",
     "iopub.execute_input": "2023-09-13T17:05:58.060541Z",
     "iopub.status.idle": "2023-09-13T17:06:06.730186Z",
     "shell.execute_reply.started": "2023-09-13T17:05:58.060493Z",
     "shell.execute_reply": "2023-09-13T17:06:06.729165Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-11-01T14:23:53.031710Z",
     "start_time": "2024-11-01T14:23:49.941666Z"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "filepath = '../Data/tinyshakespeare.txt'\n",
    "\n",
    "with open(filepath, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-13T17:06:06.732303Z",
     "iopub.execute_input": "2023-09-13T17:06:06.732998Z",
     "iopub.status.idle": "2023-09-13T17:06:06.991353Z",
     "shell.execute_reply.started": "2023-09-13T17:06:06.732963Z",
     "shell.execute_reply": "2023-09-13T17:06:06.987712Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-11-01T14:23:53.037304Z",
     "start_time": "2024-11-01T14:23:53.032724Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "# Let's display the first 100 characters of the text:\n",
    "text[:100]"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-13T17:06:06.995031Z",
     "iopub.execute_input": "2023-09-13T17:06:06.995487Z",
     "iopub.status.idle": "2023-09-13T17:06:07.003682Z",
     "shell.execute_reply.started": "2023-09-13T17:06:06.995444Z",
     "shell.execute_reply": "2023-09-13T17:06:07.002560Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-11-01T14:23:53.056576Z",
     "start_time": "2024-11-01T14:23:53.038310Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": "# Let's examine characters:\n\"\".join(sorted(set(text.lower())))",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-13T17:06:07.007036Z",
     "iopub.execute_input": "2023-09-13T17:06:07.008419Z",
     "iopub.status.idle": "2023-09-13T17:06:07.054600Z",
     "shell.execute_reply.started": "2023-09-13T17:06:07.008367Z",
     "shell.execute_reply": "2023-09-13T17:06:07.052367Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-11-01T14:23:53.069568Z",
     "start_time": "2024-11-01T14:23:53.057520Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n !$&',-.3:;?abcdefghijklmnopqrstuvwxyz\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": "# Let's take a look at the length of characters:\nlen(\"\".join(sorted(set(text.lower()))))",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-13T17:06:07.056115Z",
     "iopub.execute_input": "2023-09-13T17:06:07.056528Z",
     "iopub.status.idle": "2023-09-13T17:06:07.104505Z",
     "shell.execute_reply.started": "2023-09-13T17:06:07.056489Z",
     "shell.execute_reply": "2023-09-13T17:06:07.091363Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-11-01T14:23:53.082699Z",
     "start_time": "2024-11-01T14:23:53.070573Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": "# <b><div style='padding:15px;background-color:#850E35;color:white;border-radius:2px;font-size:110%;text-align: center'>2. Text Preprocessing</div></b>",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "This section focuses on preprocessing the raw text data. We create a TextVectorization layer that tokenizes the text at the character level and converts all characters to lowercase for consistency. The layer is adapted to the text data, allowing us to efficiently encode the text into numerical sequences. We also check the shape of the encoded text to understand its dimensions.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Let's create a TextVectorization layer for character-level tokenization:\n",
    "text_vec_layer = tf.keras.layers.TextVectorization(\n",
    "    split=\"character\", standardize=\"lower\") # character-level tokenization and lowercase conversion. Character tokenization is used to preserve the structure of the text."
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-13T17:06:07.106493Z",
     "iopub.execute_input": "2023-09-13T17:06:07.109783Z",
     "iopub.status.idle": "2023-09-13T17:06:10.296634Z",
     "shell.execute_reply.started": "2023-09-13T17:06:07.109747Z",
     "shell.execute_reply": "2023-09-13T17:06:10.295720Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-11-01T14:23:53.162188Z",
     "start_time": "2024-11-01T14:23:53.083705Z"
    }
   },
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": "# Let's adapt the TextVectorization layer to the text data:\ntext_vec_layer.adapt([text])",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-13T17:06:10.297859Z",
     "iopub.execute_input": "2023-09-13T17:06:10.298219Z",
     "iopub.status.idle": "2023-09-13T17:06:10.871227Z",
     "shell.execute_reply.started": "2023-09-13T17:06:10.298185Z",
     "shell.execute_reply": "2023-09-13T17:06:10.870203Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-11-01T14:23:53.479419Z",
     "start_time": "2024-11-01T14:23:53.162188Z"
    }
   },
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": "# Let's check the shape of the encoded text:\ntext_vec_layer([text]).shape",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-13T17:06:10.873024Z",
     "iopub.execute_input": "2023-09-13T17:06:10.873455Z",
     "iopub.status.idle": "2023-09-13T17:06:11.440342Z",
     "shell.execute_reply.started": "2023-09-13T17:06:10.873420Z",
     "shell.execute_reply": "2023-09-13T17:06:11.439304Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-11-01T14:23:53.707947Z",
     "start_time": "2024-11-01T14:23:53.480424Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 1115394])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": [
    "# Let's preprocess the text:\n",
    "encoded = text_vec_layer([text])[0]\n",
    "encoded # 0 padding, 1 unknown character"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-13T17:06:11.443643Z",
     "iopub.execute_input": "2023-09-13T17:06:11.443929Z",
     "iopub.status.idle": "2023-09-13T17:06:11.959291Z",
     "shell.execute_reply.started": "2023-09-13T17:06:11.443904Z",
     "shell.execute_reply": "2023-09-13T17:06:11.958289Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-11-01T14:23:53.927957Z",
     "start_time": "2024-11-01T14:23:53.708953Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([21,  7, 10, ..., 22, 28, 12], dtype=int64)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": "The TextVectorization layer assigns 0 for padding tokens and 1 for unknown characters. Since we currently don't need these tokens, we subtract 2 from the character IDs and calculate both the count of distinct characters and the total character count.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Let’s subtract 2 from the character IDs and compute the number of distinct characters and the total number of characters:\n",
    "encoded -= 2 # 0 padding, 1 unknown character\n",
    "n_tokens = text_vec_layer.vocabulary_size()-2 # one token is one character\n",
    "n_tokens # number of distinct characters in the text data. 39 distinct characters. 39 tokens."
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-13T17:06:11.963801Z",
     "iopub.execute_input": "2023-09-13T17:06:11.964120Z",
     "iopub.status.idle": "2023-09-13T17:06:11.976780Z",
     "shell.execute_reply.started": "2023-09-13T17:06:11.964094Z",
     "shell.execute_reply": "2023-09-13T17:06:11.975441Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-11-01T14:23:53.936942Z",
     "start_time": "2024-11-01T14:23:53.928962Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": [
    "# Let's take a look at the length of the dataset:\n",
    "dataset_size = len(encoded) # total number of characters in the text data\n",
    "dataset_size"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-13T17:06:11.978595Z",
     "iopub.execute_input": "2023-09-13T17:06:11.979180Z",
     "iopub.status.idle": "2023-09-13T17:06:11.986988Z",
     "shell.execute_reply.started": "2023-09-13T17:06:11.979144Z",
     "shell.execute_reply": "2023-09-13T17:06:11.986037Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-11-01T14:23:53.940570Z",
     "start_time": "2024-11-01T14:23:53.936942Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1115394"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "markdown",
   "source": "# <b><div style='padding:15px;background-color:#850E35;color:white;border-radius:2px;font-size:110%;text-align: center'>3. Dataset Preparation </div></b>",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "Here, we define a function called to_dataset that converts the encoded text sequences into a dataset suitable for training. This function segments the text into overlapping sequences of a specified length and organizes them into batches. Optionally, it shuffles the dataset to enhance randomness during training. An example usage of the to_dataset function is provided to illustrate its functionality.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Let's create a function to convert text sequences into a dataset\n",
    "def to_dataset(sequence, length, shuffle=False, seed=None, batch_size=32):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(sequence)\n",
    "    ds = ds.window(length + 1, shift=1, drop_remainder=True) # window size is length + 1 to create overlapping sequences of length. Because we want to predict the next character in the sequence.\n",
    "    ds = ds.flat_map(lambda window_ds: window_ds.batch(length + 1)) # flat_map() method flattens the dataset by applying the function to each element of the dataset. For example, if the dataset contains multiple windows, the flat_map() method will flatten them into a single dataset.\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(100_000, seed=seed)\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)\n",
    "\n",
    "# Above function does the following: \n",
    "# 1. It creates a dataset from the input sequence.\n",
    "# 2. It segments the dataset into overlapping windows of a specified length.\n",
    "# 3. It converts the windows into batches.\n",
    "# 4. It shuffles the dataset if needed.\n",
    "# 5. It maps the windows to input and target sequences.\n",
    "# 6. It prefetches the dataset for better performance.\n",
    "# Shift is set to 1 to create overlapping sequences. For example, if the sequence is [1, 2, 3, 4, 5] and the length is 3, the resulting sequences will be [1, 2, 3], [2, 3, 4], and [3, 4, 5]."
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-13T17:06:11.988253Z",
     "iopub.execute_input": "2023-09-13T17:06:11.989175Z",
     "iopub.status.idle": "2023-09-13T17:06:11.997326Z",
     "shell.execute_reply.started": "2023-09-13T17:06:11.989130Z",
     "shell.execute_reply": "2023-09-13T17:06:11.996296Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-11-01T14:23:53.945082Z",
     "start_time": "2024-11-01T14:23:53.940570Z"
    }
   },
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "source": "# Let's get an example and pass it to the function:\nlist(to_dataset(text_vec_layer([\"I like\"])[0], length=5))",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-13T17:06:11.998476Z",
     "iopub.execute_input": "2023-09-13T17:06:11.998964Z",
     "iopub.status.idle": "2023-09-13T17:06:12.110340Z",
     "shell.execute_reply.started": "2023-09-13T17:06:11.998933Z",
     "shell.execute_reply": "2023-09-13T17:06:12.109312Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-11-01T14:23:54.100186Z",
     "start_time": "2024-11-01T14:23:53.945591Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<tf.Tensor: shape=(1, 5), dtype=int64, numpy=array([[ 7,  2, 13,  7, 26]], dtype=int64)>,\n",
       "  <tf.Tensor: shape=(1, 5), dtype=int64, numpy=array([[ 2, 13,  7, 26,  3]], dtype=int64)>)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "source": "Let's create the training, validation and test datasets.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "length = 100\n",
    "tf.random.set_seed(42)\n",
    "# The training dataset:\n",
    "train_set = to_dataset(encoded[:1_000_000], length=length, shuffle=True, seed=42) # 1 million characters\n",
    "# The validation dataset: \n",
    "valid_set = to_dataset(encoded[1_000_000:1_060_000], length=length) # 60,000 characters\n",
    "# Test dataset:\n",
    "test_set = to_dataset(encoded[1_060_000:], length=length) # 60,000 characters"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-13T17:06:12.112291Z",
     "iopub.execute_input": "2023-09-13T17:06:12.113450Z",
     "iopub.status.idle": "2023-09-13T17:06:12.204192Z",
     "shell.execute_reply.started": "2023-09-13T17:06:12.113413Z",
     "shell.execute_reply": "2023-09-13T17:06:12.203216Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-11-01T14:23:54.160255Z",
     "start_time": "2024-11-01T14:23:54.101192Z"
    }
   },
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "source": "# <b><div style='padding:15px;background-color:#850E35;color:white;border-radius:2px;font-size:110%;text-align: center'>4. Model Definition and Training </div></b>",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "In this part of the code, we define the architecture of a neural network model for text generation. The model consists of an Embedding layer for representing tokens, a GRU (Gated Recurrent Unit) layer for sequence modeling, and a Dense layer with a softmax activation for predicting the next character. We compile the model using the sparse categorical cross-entropy loss and the Nadam optimizer. We also incorporate a ModelCheckpoint callback to save the best model weights during training. The model is then trained on the prepared datasets using the fit method.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# Let's define the model architecture:\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16), # Embedding layer is used to represent tokens. The output_dim parameter specifies the size of the embedding vectors.\n",
    "    tf.keras.layers.GRU(128, return_sequences=True),\n",
    "    tf.keras.layers.Dense(n_tokens, activation=\"softmax\") # Dense layer is used for predicting the next character. 39 output units are used to predict the probability of each character. Softmax activation is used to ensure that the output probabilities sum to 1.\n",
    "]) # Embedding layer is used to represent tokens, GRU layer is used for sequence modeling, and Dense layer is used for predicting the next character. \n",
    "# Let's compile the model:\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", # sparse_categorical_crossentropy loss is used because the targets are integer sequences.\n",
    "              optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "\n",
    "#  Let's train the model and save the best checkpoints:\n",
    "model_ckpt = tf.keras.callbacks.ModelCheckpoint(\"my_shakespeare_model.keras\", monitor=\"val_accuracy\", save_best_only=True) # ModelCheckpoint callback is used to save the best model weights during training.\n",
    "\n",
    "# Let's train the model:\n",
    "history = model.fit( train_set, validation_data=valid_set, epochs=3, callbacks=[model_ckpt])"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-13T17:06:12.206361Z",
     "iopub.execute_input": "2023-09-13T17:06:12.207053Z",
     "iopub.status.idle": "2023-09-13T17:27:36.126723Z",
     "shell.execute_reply.started": "2023-09-13T17:06:12.207018Z",
     "shell.execute_reply": "2023-09-13T17:27:36.125672Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-11-01T15:21:09.753710Z",
     "start_time": "2024-11-01T14:24:20.678301Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "  31246/Unknown \u001B[1m1176s\u001B[0m 37ms/step - accuracy: 0.5478 - loss: 1.4954"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\contextlib.py:155: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[1m31247/31247\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1193s\u001B[0m 38ms/step - accuracy: 0.5478 - loss: 1.4954 - val_accuracy: 0.5339 - val_loss: 1.6035\n",
      "Epoch 2/3\n",
      "\u001B[1m31247/31247\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m1220s\u001B[0m 39ms/step - accuracy: 0.5974 - loss: 1.2921 - val_accuracy: 0.5427 - val_loss: 1.5740\n",
      "Epoch 3/3\n",
      "\u001B[1m31247/31247\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m996s\u001B[0m 32ms/step - accuracy: 0.6024 - loss: 1.2704 - val_accuracy: 0.5442 - val_loss: 1.5688\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "source": [
    "# Let's add the text preprocessing layer:\n",
    "shakespeare_model = tf.keras.Sequential([ # Sequential model is created to combine the TextVectorization layer, character-level adjustment, and the previously trained text generation model.\n",
    "    text_vec_layer,\n",
    "    tf.keras.layers.Lambda(lambda X: X - 2), # subtract 2 from the character IDs to remove padding and unknown tokens\n",
    "    model\n",
    "])"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-13T17:27:36.128740Z",
     "iopub.execute_input": "2023-09-13T17:27:36.129686Z",
     "iopub.status.idle": "2023-09-13T17:27:36.394561Z",
     "shell.execute_reply.started": "2023-09-13T17:27:36.129657Z",
     "shell.execute_reply": "2023-09-13T17:27:36.393289Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-11-01T15:36:00.336779Z",
     "start_time": "2024-11-01T15:36:00.332299Z"
    }
   },
   "outputs": [],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "source": "# <b><div style='padding:15px;background-color:#850E35;color:white;border-radius:2px;font-size:110%;text-align: center'>5. Text Generation </div></b>",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "This section defines a higher-level model for text generation, combining the TextVectorization layer, character-level adjustment, and the previously trained text generation model. This model can be used to generate text based on an initial input.",
   "metadata": {}
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-01T15:38:38.229860Z",
     "start_time": "2024-11-01T15:38:38.164900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Let's generate text using the trained model:\n",
    "y_proba = shakespeare_model.predict([\"To be or not to b\"])[0, -1] # predict the next character in the sequence \"To be or not to b\" using the trained model. [0, -1] is used to select the last character in the sequence.\n",
    "y_pred = tf.argmax(y_proba) # get the character ID with the highest probability\n",
    "text_vec_layer.get_vocabulary()[y_pred + 2] # get the character corresponding to the character ID"
   ],
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling TextVectorization.call().\n\n\u001B[1mWhen using `TextVectorization` to tokenize strings, the input rank must be 1 or the last shape dimension must be 1. Received: inputs.shape=(1, 17) with rank=2\u001B[0m\n\nArguments received by TextVectorization.call():\n  • inputs=tf.Tensor(shape=(1, 17), dtype=string)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[31], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# Let's generate text using the trained model:\u001B[39;00m\n\u001B[0;32m      2\u001B[0m input_text \u001B[38;5;241m=\u001B[39m text_vec_layer([\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTo be or not to b\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m----> 3\u001B[0m y_proba \u001B[38;5;241m=\u001B[39m shakespeare_model\u001B[38;5;241m.\u001B[39mpredict(input_text)[\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;66;03m# predict the next character in the sequence \"To be or not to b\" using the trained model. [0, -1] is used to select the last character in the sequence.\u001B[39;00m\n\u001B[0;32m      4\u001B[0m y_pred \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39margmax(y_proba) \u001B[38;5;66;03m# get the character ID with the highest probability\u001B[39;00m\n\u001B[0;32m      5\u001B[0m text_vec_layer\u001B[38;5;241m.\u001B[39mget_vocabulary()[y_pred \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m2\u001B[39m]\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    119\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m    120\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m    121\u001B[0m     \u001B[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m--> 122\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    123\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    124\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\preprocessing\\text_vectorization.py:542\u001B[0m, in \u001B[0;36mTextVectorization._preprocess\u001B[1;34m(self, inputs)\u001B[0m\n\u001B[0;32m    540\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m inputs\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;241m.\u001B[39mrank \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m    541\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m inputs\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m--> 542\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    543\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWhen using `TextVectorization` to tokenize strings, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    544\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mthe input rank must be 1 or the last shape dimension \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    545\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmust be 1. Received: inputs.shape=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00minputs\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    546\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwith rank=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00minputs\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;241m.\u001B[39mrank\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    547\u001B[0m         )\n\u001B[0;32m    548\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    549\u001B[0m         inputs \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39msqueeze(inputs, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "\u001B[1;31mValueError\u001B[0m: Exception encountered when calling TextVectorization.call().\n\n\u001B[1mWhen using `TextVectorization` to tokenize strings, the input rank must be 1 or the last shape dimension must be 1. Received: inputs.shape=(1, 17) with rank=2\u001B[0m\n\nArguments received by TextVectorization.call():\n  • inputs=tf.Tensor(shape=(1, 17), dtype=string)"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "markdown",
   "source": "# <b><div style='padding:15px;background-color:#850E35;color:white;border-radius:2px;font-size:110%;text-align: center'>6. Text Generation Functions</div></b>",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": "Here, we define two important functions for text generation. The next_char function predicts the next character in a sequence given a context and a temperature parameter that controls the randomness of predictions. The extend_text function extends a given text with additional characters by iteratively predicting the next character based on the context. Example usages of these functions are provided to demonstrate how to generate text with different temperatures.",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# How to use the tf.random.categorical() method:\n",
    "log_probas = tf.math.log([[0.6, 0.3, 0.1]]) # log probabilities of three classes (0.6, 0.3, 0.1) are calculated. Log probabilities are used to prevent numerical instability. 0.6 is the highest probability. 0.6 is probability of class 0, 0.3 is probability of class 1, 0.1 is probability of class 2.\n",
    "tf.random.set_seed(42)\n",
    "tf.random.categorical(log_probas, num_samples=10) # 10 samples are drawn from the log probabilities. The output is the indices of the classes with the highest probability."
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-13T17:27:36.905349Z",
     "iopub.execute_input": "2023-09-13T17:27:36.906354Z",
     "iopub.status.idle": "2023-09-13T17:27:36.935999Z",
     "shell.execute_reply.started": "2023-09-13T17:27:36.906320Z",
     "shell.execute_reply": "2023-09-13T17:27:36.934902Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-11-01T15:38:41.554647Z",
     "start_time": "2024-11-01T15:38:41.544063Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 10), dtype=int64, numpy=array([[0, 1, 0, 2, 1, 0, 0, 0, 0, 0]], dtype=int64)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "code",
   "source": [
    "def next_char(text, temperature=1): # the next_char() function generates the next character in the sequence given a context and a temperature parameter.\n",
    "    y_proba = shakespeare_model.predict([text])[0, -1:] # predict the next character in the sequence using the trained model. Probabilities of all characters are calculated.\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature # the logits are rescaled using the temperature parameter. A higher temperature increases the randomness of the predictions.\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1)[0, 0] # the character ID with the highest probability is selected. \n",
    "    return text_vec_layer.get_vocabulary()[char_id + 2] # the character corresponding to the character ID is returned."
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-13T17:27:36.937584Z",
     "iopub.execute_input": "2023-09-13T17:27:36.938171Z",
     "iopub.status.idle": "2023-09-13T17:27:36.945437Z",
     "shell.execute_reply.started": "2023-09-13T17:27:36.938137Z",
     "shell.execute_reply": "2023-09-13T17:27:36.944460Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-11-01T15:38:42.684174Z",
     "start_time": "2024-11-01T15:38:42.680372Z"
    }
   },
   "outputs": [],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "source": [
    "def extend_text(text, n_chars=50, temperature=1): # the extend_text() function generates text by predicting the next character in the sequence iteratively.\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    return text"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-13T17:27:36.946876Z",
     "iopub.execute_input": "2023-09-13T17:27:36.947512Z",
     "iopub.status.idle": "2023-09-13T17:27:36.954793Z",
     "shell.execute_reply.started": "2023-09-13T17:27:36.947480Z",
     "shell.execute_reply": "2023-09-13T17:27:36.953790Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-11-01T15:36:43.991691Z",
     "start_time": "2024-11-01T15:36:43.987720Z"
    }
   },
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "source": [
    "# Let's generate a text with a low temperature:\n",
    "tf.random.set_seed(42)\n",
    "print(extend_text(\"I like\", temperature=0.01))"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-13T17:27:36.957476Z",
     "iopub.execute_input": "2023-09-13T17:27:36.958067Z",
     "iopub.status.idle": "2023-09-13T17:27:41.052891Z",
     "shell.execute_reply.started": "2023-09-13T17:27:36.958035Z",
     "shell.execute_reply": "2023-09-13T17:27:41.051265Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-11-01T15:39:07.957957Z",
     "start_time": "2024-11-01T15:39:07.849547Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yasin\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: keras_tensor_4. Received: the structure of inputs=('*',)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling TextVectorization.call().\n\n\u001B[1mWhen using `TextVectorization` to tokenize strings, the input rank must be 1 or the last shape dimension must be 1. Received: inputs.shape=(1, 6) with rank=2\u001B[0m\n\nArguments received by TextVectorization.call():\n  • inputs=tf.Tensor(shape=(1, 6), dtype=string)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[35], line 5\u001B[0m\n\u001B[0;32m      2\u001B[0m input_text \u001B[38;5;241m=\u001B[39m text_vec_layer([\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mI like\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[0;32m      4\u001B[0m tf\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mset_seed(\u001B[38;5;241m42\u001B[39m)\n\u001B[1;32m----> 5\u001B[0m \u001B[38;5;28mprint\u001B[39m(extend_text(input_text, temperature\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.01\u001B[39m))\n",
      "Cell \u001B[1;32mIn[24], line 3\u001B[0m, in \u001B[0;36mextend_text\u001B[1;34m(text, n_chars, temperature)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mextend_text\u001B[39m(text, n_chars\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m50\u001B[39m, temperature\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m): \u001B[38;5;66;03m# the extend_text() function generates text by predicting the next character in the sequence iteratively.\u001B[39;00m\n\u001B[0;32m      2\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(n_chars):\n\u001B[1;32m----> 3\u001B[0m         text \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m next_char(text, temperature)\n\u001B[0;32m      4\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m text\n",
      "Cell \u001B[1;32mIn[33], line 2\u001B[0m, in \u001B[0;36mnext_char\u001B[1;34m(text, temperature)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mnext_char\u001B[39m(text, temperature\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m): \u001B[38;5;66;03m# the next_char() function generates the next character in the sequence given a context and a temperature parameter.\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m     y_proba \u001B[38;5;241m=\u001B[39m shakespeare_model\u001B[38;5;241m.\u001B[39mpredict([text])[\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m:] \u001B[38;5;66;03m# predict the next character in the sequence using the trained model. Probabilities of all characters are calculated.\u001B[39;00m\n\u001B[0;32m      3\u001B[0m     rescaled_logits \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mmath\u001B[38;5;241m.\u001B[39mlog(y_proba) \u001B[38;5;241m/\u001B[39m temperature \u001B[38;5;66;03m# the logits are rescaled using the temperature parameter. A higher temperature increases the randomness of the predictions.\u001B[39;00m\n\u001B[0;32m      4\u001B[0m     char_id \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mrandom\u001B[38;5;241m.\u001B[39mcategorical(rescaled_logits, num_samples\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)[\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m0\u001B[39m] \u001B[38;5;66;03m# the character ID with the highest probability is selected. \u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    119\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n\u001B[0;32m    120\u001B[0m     \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[0;32m    121\u001B[0m     \u001B[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001B[39;00m\n\u001B[1;32m--> 122\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\u001B[38;5;241m.\u001B[39mwith_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    123\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    124\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "File \u001B[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\preprocessing\\text_vectorization.py:542\u001B[0m, in \u001B[0;36mTextVectorization._preprocess\u001B[1;34m(self, inputs)\u001B[0m\n\u001B[0;32m    540\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m inputs\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;241m.\u001B[39mrank \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m    541\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m inputs\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m--> 542\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    543\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWhen using `TextVectorization` to tokenize strings, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    544\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mthe input rank must be 1 or the last shape dimension \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    545\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmust be 1. Received: inputs.shape=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00minputs\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    546\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwith rank=\u001B[39m\u001B[38;5;132;01m{\u001B[39;00minputs\u001B[38;5;241m.\u001B[39mshape\u001B[38;5;241m.\u001B[39mrank\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    547\u001B[0m         )\n\u001B[0;32m    548\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    549\u001B[0m         inputs \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39msqueeze(inputs, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n",
      "\u001B[1;31mValueError\u001B[0m: Exception encountered when calling TextVectorization.call().\n\n\u001B[1mWhen using `TextVectorization` to tokenize strings, the input rank must be 1 or the last shape dimension must be 1. Received: inputs.shape=(1, 6) with rank=2\u001B[0m\n\nArguments received by TextVectorization.call():\n  • inputs=tf.Tensor(shape=(1, 6), dtype=string)"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "source": "# Let's create a higher temperature text:\nprint(extend_text(\"I like\", temperature=1))",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2023-09-13T17:27:41.054144Z",
     "iopub.execute_input": "2023-09-13T17:27:41.054522Z",
     "iopub.status.idle": "2023-09-13T17:27:44.826496Z",
     "shell.execute_reply.started": "2023-09-13T17:27:41.054480Z",
     "shell.execute_reply": "2023-09-13T17:27:44.825554Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": "# <b><div style='padding:15px;background-color:#850E35;color:white;border-radius:2px;font-size:110%;text-align: center'>Conclusion</div></b>\n\nIn this notebook, we covered how to build a RNN-based model with TensorFlow for text generation.\n\nThanks for reading. If you enjoy this notebook, don't forget upvote. ",
   "metadata": {}
  }
 ]
}
