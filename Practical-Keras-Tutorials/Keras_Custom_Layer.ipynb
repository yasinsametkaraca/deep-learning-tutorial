{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyOh77GXvrKiJcs7apkq5UEU",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow import keras"
   ],
   "metadata": {
    "id": "-khFwRQWV8Ps"
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Custom Layer with Subclassing API"
   ],
   "metadata": {
    "id": "jqEafbT7n2hS"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class Linear(keras.layers.Layer): # This is the custom layer class. It inherits from the Layer class of the keras.layers module\n",
    "  def __init__(self, units=32, input_dim = 32): # units is the number of neurons in the layer. input_dim is the number of features in the input\n",
    "    super(Linear,self).__init__() # This is the constructor of the parent class\n",
    "\n",
    "    w_init = tf.random_normal_initializer() # This is the initializer for the weights of the layer\n",
    "    self.w = tf.Variable(\n",
    "        initial_value=w_init(shape=(input_dim,units), dtype=\"float32\"),\n",
    "        trainable=True \n",
    "    )\n",
    "    b_init = tf.zeros_initializer() # This is the initializer for the bias of the layer. Zeros is a good choice for the bias\n",
    "    self.b = tf.Variable(\n",
    "        initial_value=b_init(shape=(units,), dtype=\"float32\"),\n",
    "        trainable=True\n",
    "    )\n",
    "  def call(self, inputs): \n",
    "    return tf.matmul(inputs, self.w) + self.b # This is the forward pass of the layer. Mathematically, it is the dot product of the input and the weights plus the bias. Matmul is the matrix multiplication function in tensorflow"
   ],
   "metadata": {
    "id": "LwdhJR5ZWFNc"
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "x = tf.ones((2,2))\n",
    "x "
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oPFVXzv8d3l9",
    "outputId": "25294ca0-7d08-4fbd-c3dd-a41ee0a4fce0"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2), dtype=float32, numpy=\n",
       "array([[1., 1.],\n",
       "       [1., 1.]], dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "linear_layer = Linear(4,2)\n",
    "y = linear_layer(x) # call() method is called when the object is called like a function. \n",
    "y"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MEE1JLLLeCNk",
    "outputId": "7d58c46b-8c84-4793-e27f-e3fbc72f2982"
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
       "array([[ 0.06356013, -0.02136349,  0.0958024 ,  0.04870867],\n",
       "       [ 0.06356013, -0.02136349,  0.0958024 ,  0.04870867]],\n",
       "      dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "linear_layer.weights"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rk8i9HfqeVgF",
    "outputId": "c7ee3423-03ad-4164-ff9f-b4d3dfb815b8"
   },
   "execution_count": 5,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<tf.Variable 'Variable:0' shape=(2, 4) dtype=float32, numpy=\n",
       " array([[ 0.08850016, -0.0294551 ,  0.04489021,  0.01145601],\n",
       "        [-0.02494004,  0.0080916 ,  0.0509122 ,  0.03725266]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'Variable:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>]"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Custom Layer using add_weight"
   ],
   "metadata": {
    "id": "Mj4KbWSwoQWe"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from tensorflow.python.ops.init_ops_v2 import Initializer\n",
    "class Linear(keras.layers.Layer):\n",
    "  def __init__(self, units=32, input_dim = 32):\n",
    "    super(Linear, self).__init__()\n",
    "    self.w = self.add_weight(shape=(input_dim, units),\n",
    "                             initializer=\"random_normal\",\n",
    "                             trainable = True)\n",
    "    self.b = self.add_weight(shape=(units),\n",
    "                             initializer = \"zeros\",\n",
    "                             trainable = True)\n",
    "  def call(self,inputs):\n",
    "    return tf.matmul(inputs, self.w) + self.b  "
   ],
   "metadata": {
    "id": "-LFN4lnveqie"
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "x = tf.ones((2,2))\n",
    "linear_layer = Linear(4,2)\n",
    "y = linear_layer(x)"
   ],
   "metadata": {
    "id": "H1HwqrTvgOZl"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "y"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R1tyK5_PgUh0",
    "outputId": "70b0980e-c742-4aa2-cba7-6cb16d8dec9c"
   },
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 4), dtype=float32, numpy=\n",
       "array([[0.07724835, 0.0008499 , 0.106326  , 0.02716313],\n",
       "       [0.07724835, 0.0008499 , 0.106326  , 0.02716313]], dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Custom Layer without the shape of the inputs"
   ],
   "metadata": {
    "id": "tnjaMaZqoevF"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class Linear(keras.layers.Layer):\n",
    "  def __init__(self, units=32):\n",
    "    super(Linear,self).__init__()\n",
    "    self.units = units\n",
    "  def build(self, input_shape): # This method is called when the layer is first called. It is used to create the weights of the layer\n",
    "    self.w = self.add_weight(shape=(input_shape[-1],self.units),\n",
    "                             initializer = \"random_normal\",\n",
    "                             trainable = True)\n",
    "    self.b = self.add_weight(shape = (self.units,),\n",
    "                             initializer = \"random_normal\",\n",
    "                             trainable = True)\n",
    "  def call(self, inputs):\n",
    "    return tf.matmul(inputs, self.w) + self.b               "
   ],
   "metadata": {
    "id": "KX2pvzGTgWal"
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "linear_layer = Linear(32)\n",
    "y = linear_layer(x)\n",
    "y"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p0smwT4YlEm3",
    "outputId": "c25b8361-67f0-4c42-8e1a-390df15c1cb5"
   },
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 32), dtype=float32, numpy=\n",
       "array([[ 0.12529299, -0.0561902 ,  0.03834991, -0.07893594,  0.00859061,\n",
       "         0.08317355, -0.05678261,  0.0037364 , -0.04070574, -0.12113686,\n",
       "         0.03729274,  0.01368941, -0.05131226,  0.03087646, -0.02225749,\n",
       "         0.04070945,  0.01450459,  0.08193067,  0.0622445 ,  0.1920329 ,\n",
       "        -0.0106803 , -0.07432184,  0.003447  ,  0.11814891, -0.07613041,\n",
       "         0.01972482, -0.08320747,  0.13414364,  0.00047702, -0.15275721,\n",
       "         0.09923995, -0.06166402],\n",
       "       [ 0.12529299, -0.0561902 ,  0.03834991, -0.07893594,  0.00859061,\n",
       "         0.08317355, -0.05678261,  0.0037364 , -0.04070574, -0.12113686,\n",
       "         0.03729274,  0.01368941, -0.05131226,  0.03087646, -0.02225749,\n",
       "         0.04070945,  0.01450459,  0.08193067,  0.0622445 ,  0.1920329 ,\n",
       "        -0.0106803 , -0.07432184,  0.003447  ,  0.11814891, -0.07613041,\n",
       "         0.01972482, -0.08320747,  0.13414364,  0.00047702, -0.15275721,\n",
       "         0.09923995, -0.06166402]], dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Layers are recursively composable"
   ],
   "metadata": {
    "id": "HJJFizFBouXh"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class MLPBlock(keras.layers.Layer): # This is a custom layer that is a block of a multi-layer perceptron\n",
    "  def __init__(self):\n",
    "    super(MLPBlock, self).__init__()\n",
    "    self.linear_1 = Linear(32) # This is the first layer of the block. It is the input layer of the block\n",
    "    self.linear_2 = Linear(32) # This is the second layer of the block. It is the hidden layer of the block\n",
    "    self.linear_3 = Linear(1) # This is the third layer of the block. It is the output layer of the block\n",
    "  def call(self, inputs):\n",
    "    x = self.linear_1(inputs) # This is the forward pass of the block. It is the dot product of the input and the weights of the first layer plus the bias of the first layer\n",
    "    x = tf.nn.relu(x) # This is the activation function of the first layer. It is the ReLU activation function\n",
    "    x = self.linear_2(x) \n",
    "    x = tf.nn.relu(x)\n",
    "    return self.linear_3(x)"
   ],
   "metadata": {
    "id": "vDo0G44WlNXA"
   },
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "mlp = MLPBlock() \n",
    "y = mlp(tf.ones(shape=(3,64))) # All variables are one. The shape of the input is (3,64). 3 is the number of samples and 64 is the number of features"
   ],
   "metadata": {
    "id": "jqI06r8Yme6L"
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "mlp.weights"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GFEzhYxknDiN",
    "outputId": "560e0c89-90d1-4252-833e-212d352b0f7e"
   },
   "execution_count": 17,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<tf.Variable 'mlp_block/linear_5/Variable:0' shape=(64, 32) dtype=float32, numpy=\n",
       " array([[ 0.03181227, -0.03552845, -0.00532066, ...,  0.12079153,\n",
       "         -0.07475913,  0.0099043 ],\n",
       "        [ 0.0015341 , -0.02581516, -0.03239488, ...,  0.0370282 ,\n",
       "          0.02811593, -0.03380568],\n",
       "        [-0.00715649,  0.0582541 ,  0.04256558, ...,  0.0377348 ,\n",
       "          0.10212301, -0.03072884],\n",
       "        ...,\n",
       "        [ 0.05020842, -0.10764688,  0.00189259, ..., -0.0117731 ,\n",
       "          0.02547028, -0.06487002],\n",
       "        [-0.00168144, -0.01505039, -0.0209573 , ..., -0.01630844,\n",
       "         -0.0164077 , -0.07637613],\n",
       "        [-0.05147887,  0.09903259,  0.01605012, ...,  0.02922877,\n",
       "          0.0156203 ,  0.07475672]], dtype=float32)>,\n",
       " <tf.Variable 'mlp_block/linear_5/Variable:0' shape=(32,) dtype=float32, numpy=\n",
       " array([-0.01699487,  0.01961638,  0.04468011, -0.07827999,  0.06531694,\n",
       "         0.04750273, -0.01846105, -0.04750106,  0.04930599,  0.03066118,\n",
       "        -0.13191342, -0.00432053,  0.04816367,  0.11426394,  0.01057771,\n",
       "         0.07847259, -0.05077245,  0.08985738,  0.02169767, -0.02449624,\n",
       "         0.00086025,  0.01822815, -0.01215468,  0.01937916, -0.0667214 ,\n",
       "         0.05271372,  0.02958361, -0.08584312, -0.00931098, -0.07165599,\n",
       "        -0.05130129,  0.01846607], dtype=float32)>,\n",
       " <tf.Variable 'mlp_block/linear_6/Variable:0' shape=(32, 32) dtype=float32, numpy=\n",
       " array([[-0.02221624,  0.0157703 , -0.09380899, ...,  0.00060783,\n",
       "         -0.03553248,  0.04492274],\n",
       "        [-0.0453898 ,  0.04680035, -0.00126955, ..., -0.12641528,\n",
       "         -0.07855913,  0.14558148],\n",
       "        [ 0.04044304,  0.00720663,  0.05782167, ..., -0.01838308,\n",
       "          0.06212392, -0.04211275],\n",
       "        ...,\n",
       "        [ 0.03310738, -0.01621311, -0.01417775, ..., -0.00786952,\n",
       "         -0.0022546 , -0.00367655],\n",
       "        [-0.00298777, -0.00904764,  0.00568953, ..., -0.00734057,\n",
       "         -0.00685754,  0.00986677],\n",
       "        [ 0.05762213, -0.08290815,  0.02106537, ..., -0.05110553,\n",
       "         -0.01858347, -0.03406302]], dtype=float32)>,\n",
       " <tf.Variable 'mlp_block/linear_6/Variable:0' shape=(32,) dtype=float32, numpy=\n",
       " array([ 3.84690352e-02,  2.22683232e-02,  6.22299723e-02,  2.43884604e-02,\n",
       "        -1.23071432e-01,  2.81480588e-02, -4.24688449e-03,  5.41606313e-03,\n",
       "        -7.70173743e-02,  1.09227411e-01, -5.68242259e-02,  1.64381247e-02,\n",
       "         6.62443563e-02,  4.33694310e-02, -9.74744931e-03,  1.70842752e-01,\n",
       "        -3.65027152e-02, -1.76580232e-02,  5.82955852e-02,  5.76585233e-02,\n",
       "         7.50860153e-03, -1.22458208e-02,  1.04298726e-01, -4.60303202e-02,\n",
       "         4.42866273e-02,  8.35768878e-02, -9.37250853e-02, -5.93547821e-02,\n",
       "        -3.33067030e-02, -6.47239909e-02,  5.91677837e-02,  1.30823159e-04],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'mlp_block/linear_7/Variable:0' shape=(32, 1) dtype=float32, numpy=\n",
       " array([[ 0.00171713],\n",
       "        [-0.0743994 ],\n",
       "        [ 0.09536626],\n",
       "        [ 0.00669933],\n",
       "        [ 0.03597319],\n",
       "        [ 0.02272107],\n",
       "        [-0.00487909],\n",
       "        [-0.00037978],\n",
       "        [-0.03586007],\n",
       "        [ 0.06122528],\n",
       "        [ 0.0028551 ],\n",
       "        [ 0.01687561],\n",
       "        [-0.05479074],\n",
       "        [-0.03746495],\n",
       "        [-0.08481766],\n",
       "        [-0.0068677 ],\n",
       "        [-0.06497825],\n",
       "        [-0.0241901 ],\n",
       "        [ 0.06813302],\n",
       "        [-0.05354276],\n",
       "        [ 0.01562777],\n",
       "        [ 0.08468118],\n",
       "        [ 0.01549252],\n",
       "        [ 0.0050349 ],\n",
       "        [ 0.00017409],\n",
       "        [-0.00158865],\n",
       "        [-0.04847511],\n",
       "        [-0.0029436 ],\n",
       "        [ 0.03253867],\n",
       "        [ 0.02054727],\n",
       "        [ 0.0039409 ],\n",
       "        [ 0.05293854]], dtype=float32)>,\n",
       " <tf.Variable 'mlp_block/linear_7/Variable:0' shape=(1,) dtype=float32, numpy=array([-0.0388063], dtype=float32)>]"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ]
  }
 ]
}
